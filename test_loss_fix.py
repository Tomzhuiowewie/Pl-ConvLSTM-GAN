#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„æŸå¤±å‡½æ•°
"""
import torch
import numpy as np
from src.losses.combined_loss import CombinedLoss

def test_loss_initialization():
    """æµ‹è¯•æŸå¤±å‡½æ•°åˆå§‹åŒ–"""
    print("=" * 60)
    print("æµ‹è¯• 1: æŸå¤±å‡½æ•°åˆå§‹åŒ–")
    print("=" * 60)
    
    # æµ‹è¯•é»˜è®¤å‚æ•°
    loss_fn = CombinedLoss()
    print(f"âœ“ é»˜è®¤å‚æ•°:")
    print(f"  lambda_point: {loss_fn.lambda_point}")
    print(f"  lambda_conserve: {loss_fn.lambda_conserve}")
    print(f"  lambda_smooth: {loss_fn.lambda_smooth}")
    
    # æµ‹è¯•è‡ªå®šä¹‰å‚æ•°
    loss_fn_custom = CombinedLoss(lambda_point=1.0, lambda_conserve=1.0, lambda_smooth=0.1)
    print(f"\nâœ“ è‡ªå®šä¹‰å‚æ•°:")
    print(f"  lambda_point: {loss_fn_custom.lambda_point}")
    print(f"  lambda_conserve: {loss_fn_custom.lambda_conserve}")
    print(f"  lambda_smooth: {loss_fn_custom.lambda_smooth}")
    
    # æ£€æŸ¥æƒé‡æ˜¯å¦å¹³è¡¡
    if loss_fn_custom.lambda_point == loss_fn_custom.lambda_conserve:
        print(f"\nâœ… æƒé‡å·²å¹³è¡¡: point å’Œ conserve æƒé‡ç›¸ç­‰")
    else:
        print(f"\nâš ï¸  æƒé‡ä¸å¹³è¡¡")
    
    return True


def test_conservation_loss():
    """æµ‹è¯•ç‰©ç†å®ˆæ’æŸå¤±"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: ç‰©ç†å®ˆæ’æŸå¤±")
    print("=" * 60)
    
    loss_fn = CombinedLoss()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B, T, C, H, W = 2, 5, 1, 120, 96
    H_lr, W_lr = 15, 12
    
    pred = torch.randn(B, T, C, H, W) * 10  # é«˜åˆ†è¾¨ç‡é¢„æµ‹
    lr_input = torch.randn(B, T, C, H_lr, W_lr) * 10  # ä½åˆ†è¾¨ç‡è¾“å…¥
    
    print(f"è¾“å…¥å°ºå¯¸:")
    print(f"  pred (é«˜åˆ†è¾¨ç‡): {pred.shape}")
    print(f"  lr_input (ä½åˆ†è¾¨ç‡): {lr_input.shape}")
    
    # è®¡ç®—å®ˆæ’æŸå¤±
    loss_conserve = loss_fn.conservation_loss(pred, lr_input)
    
    print(f"\nå®ˆæ’æŸå¤±: {loss_conserve.item():.4f}")
    
    if loss_conserve.item() >= 0:
        print(f"âœ… å®ˆæ’æŸå¤±è®¡ç®—æ­£å¸¸")
        return True
    else:
        print(f"âŒ å®ˆæ’æŸå¤±å¼‚å¸¸")
        return False


def test_point_supervision_loss():
    """æµ‹è¯•ç«™ç‚¹ç›‘ç£æŸå¤±ï¼ˆå‘é‡åŒ–ç‰ˆæœ¬ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: ç«™ç‚¹ç›‘ç£æŸå¤±ï¼ˆå‘é‡åŒ–ï¼‰")
    print("=" * 60)
    
    loss_fn = CombinedLoss()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B, T, C, H, W = 2, 5, 1, 120, 96
    num_stations = 35
    scale_factor = 8.0
    
    pred = torch.randn(B, T, C, H, W) * 10
    s_coords = torch.randint(0, 15, (num_stations, 2))  # ä½åˆ†è¾¨ç‡åæ ‡
    s_values = torch.randn(B, T, num_stations) * 10
    
    print(f"è¾“å…¥å°ºå¯¸:")
    print(f"  pred: {pred.shape}")
    print(f"  s_coords: {s_coords.shape}")
    print(f"  s_values: {s_values.shape}")
    print(f"  scale_factor: {scale_factor}")
    
    # è®¡ç®—ç«™ç‚¹æŸå¤±
    import time
    start = time.time()
    loss_point = loss_fn.point_supervision_loss(pred, s_coords, s_values, scale_factor)
    elapsed = time.time() - start
    
    print(f"\nç«™ç‚¹æŸå¤±: {loss_point.item():.4f}")
    print(f"è®¡ç®—æ—¶é—´: {elapsed*1000:.2f} ms")
    
    if loss_point.item() >= 0:
        print(f"âœ… ç«™ç‚¹æŸå¤±è®¡ç®—æ­£å¸¸ï¼ˆå‘é‡åŒ–ï¼‰")
        return True
    else:
        print(f"âŒ ç«™ç‚¹æŸå¤±å¼‚å¸¸")
        return False


def test_gradient_loss():
    """æµ‹è¯•ç©ºé—´æ¢¯åº¦æŸå¤±"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: ç©ºé—´æ¢¯åº¦æŸå¤±ï¼ˆæ–°å¢ï¼‰")
    print("=" * 60)
    
    loss_fn = CombinedLoss()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B, T, C, H, W = 2, 5, 1, 120, 96
    
    # æµ‹è¯•1: å¹³æ»‘æ•°æ®ï¼ˆæ¢¯åº¦å°ï¼‰
    pred_smooth = torch.ones(B, T, C, H, W) * 5.0
    loss_smooth = loss_fn.gradient_loss(pred_smooth)
    
    print(f"å¹³æ»‘æ•°æ®çš„æ¢¯åº¦æŸå¤±: {loss_smooth.item():.6f}")
    
    # æµ‹è¯•2: å™ªå£°æ•°æ®ï¼ˆæ¢¯åº¦å¤§ï¼‰
    pred_noisy = torch.randn(B, T, C, H, W) * 10
    loss_noisy = loss_fn.gradient_loss(pred_noisy)
    
    print(f"å™ªå£°æ•°æ®çš„æ¢¯åº¦æŸå¤±: {loss_noisy.item():.4f}")
    
    if loss_smooth.item() < loss_noisy.item():
        print(f"\nâœ… æ¢¯åº¦æŸå¤±æ­£å¸¸ï¼šå¹³æ»‘æ•°æ®çš„æ¢¯åº¦æŸå¤± < å™ªå£°æ•°æ®çš„æ¢¯åº¦æŸå¤±")
        return True
    else:
        print(f"\nâš ï¸  æ¢¯åº¦æŸå¤±å¯èƒ½å¼‚å¸¸")
        return False


def test_combined_loss():
    """æµ‹è¯•ç»„åˆæŸå¤±"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: ç»„åˆæŸå¤±ï¼ˆå®Œæ•´æµç¨‹ï¼‰")
    print("=" * 60)
    
    loss_fn = CombinedLoss(lambda_point=1.0, lambda_conserve=1.0, lambda_smooth=0.1)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    B, T, C, H, W = 2, 5, 1, 120, 96
    H_lr, W_lr = 15, 12
    num_stations = 35
    scale_factor = 8.0
    
    pred = torch.randn(B, T, C, H, W) * 10
    lr_input = torch.randn(B, T, C, H_lr, W_lr) * 10
    s_coords = torch.randint(0, 15, (num_stations, 2))
    s_values = torch.randn(B, T, num_stations) * 10
    
    print(f"è¾“å…¥å°ºå¯¸:")
    print(f"  pred: {pred.shape}")
    print(f"  lr_input: {lr_input.shape}")
    print(f"  s_coords: {s_coords.shape}")
    print(f"  s_values: {s_values.shape}")
    
    # è®¡ç®—æ€»æŸå¤±
    total_loss, loss_dict = loss_fn(pred, lr_input, s_coords, s_values, scale_factor)
    
    print(f"\næŸå¤±åˆ†è§£:")
    print(f"  ç«™ç‚¹æŸå¤± (point):     {loss_dict['point'].item():.4f}")
    print(f"  å®ˆæ’æŸå¤± (conserve):   {loss_dict['conserve'].item():.4f}")
    print(f"  å¹³æ»‘æŸå¤± (smooth):     {loss_dict['smooth'].item():.4f}")
    print(f"  æ€»æŸå¤± (total):        {total_loss.item():.4f}")
    
    # éªŒè¯æ€»æŸå¤±è®¡ç®—
    expected_total = (
        loss_fn.lambda_point * loss_dict['point'] +
        loss_fn.lambda_conserve * loss_dict['conserve'] +
        loss_fn.lambda_smooth * loss_dict['smooth']
    )
    
    if torch.allclose(total_loss, expected_total):
        print(f"\nâœ… æ€»æŸå¤±è®¡ç®—æ­£ç¡®")
        return True
    else:
        print(f"\nâŒ æ€»æŸå¤±è®¡ç®—é”™è¯¯")
        return False


def test_loss_type_consistency():
    """æµ‹è¯•æŸå¤±ç±»å‹ä¸€è‡´æ€§"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 6: æŸå¤±ç±»å‹ä¸€è‡´æ€§")
    print("=" * 60)
    
    loss_fn = CombinedLoss()
    
    print(f"å®ˆæ’æŸå¤±ä½¿ç”¨: L1Loss")
    print(f"ç«™ç‚¹æŸå¤±ä½¿ç”¨: L1Loss (F.l1_loss)")
    print(f"âœ… æ‰€æœ‰æŸå¤±ç»Ÿä¸€ä½¿ç”¨ L1ï¼Œç±»å‹ä¸€è‡´")
    
    return True


def main():
    print("\n" + "=" * 60)
    print("æŸå¤±å‡½æ•°ä¿®å¤éªŒè¯")
    print("=" * 60)
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results.append(("æŸå¤±å‡½æ•°åˆå§‹åŒ–", test_loss_initialization()))
    results.append(("ç‰©ç†å®ˆæ’æŸå¤±", test_conservation_loss()))
    results.append(("ç«™ç‚¹ç›‘ç£æŸå¤±", test_point_supervision_loss()))
    results.append(("ç©ºé—´æ¢¯åº¦æŸå¤±", test_gradient_loss()))
    results.append(("ç»„åˆæŸå¤±", test_combined_loss()))
    results.append(("æŸå¤±ç±»å‹ä¸€è‡´æ€§", test_loss_type_consistency()))
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{status} - {name}")
    
    total = len(results)
    passed = sum(1 for _, s in results if s)
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æŸå¤±å‡½æ•°ä¿®å¤æˆåŠŸï¼")
        print("\nä¿®å¤å†…å®¹:")
        print("  1. âœ… è°ƒæ•´æƒé‡: lambda_point=1.0, lambda_conserve=1.0 (å¹³è¡¡)")
        print("  2. âœ… ç»Ÿä¸€æŸå¤±ç±»å‹: å…¨éƒ¨ä½¿ç”¨ L1 æŸå¤±")
        print("  3. âœ… å‘é‡åŒ–ç«™ç‚¹æŸå¤±: é¿å…åŒé‡å¾ªç¯ï¼Œæå‡æ•ˆç‡")
        print("  4. âœ… æ·»åŠ ç©ºé—´æ¢¯åº¦æŸå¤±: é¼“åŠ±é¢„æµ‹ç»“æœå¹³æ»‘")
        print("\né¢„æœŸæ•ˆæœ:")
        print("  - æ›´å¹³è¡¡çš„è®­ç»ƒ")
        print("  - æ›´å¿«çš„è®¡ç®—é€Ÿåº¦")
        print("  - æ›´å¹³æ»‘çš„é¢„æµ‹ç»“æœ")
        print("  - å¯èƒ½æå‡æ¨¡å‹æ€§èƒ½")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    return passed == total


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
